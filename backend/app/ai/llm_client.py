"""
LLM Client - OpenAI Integration

Cliente para comunica√ß√£o com OpenAI (GPT-4o-mini / GPT-4.1).
Suporta function calling, sele√ß√£o inteligente de modelo, streaming.
"""
import logging
import time
from typing import Dict, Any, List, Optional, Callable
from openai import AsyncOpenAI
import json

logger = logging.getLogger(__name__)


class LLMClient:
    """
    Cliente OpenAI com sele√ß√£o inteligente de modelo.
    """
    
    def __init__(self, api_key: str):
        self.client = AsyncOpenAI(api_key=api_key)
        self.default_model = "gpt-4o-mini"
        self.advanced_model = "gpt-4-turbo-preview"  # ou gpt-4.1 quando dispon√≠vel
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 500,
        functions: Optional[List[Dict]] = None,
        function_call: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Chamada de chat completion.
        
        Args:
            messages: Lista de mensagens (system, user, assistant)
            model: Modelo a usar (None = auto-select)
            temperature: Criatividade (0.0-2.0)
            max_tokens: M√°ximo de tokens na resposta
            functions: Lista de fun√ß√µes dispon√≠veis (function calling)
            function_call: "auto", "none", ou {"name": "function_name"}
            
        Returns:
            Response completo com m√©tricas
        """
        start_time = time.time()
        
        try:
            # Selecionar modelo
            model = model or self._select_model(messages)
            
            # Preparar kwargs
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            # Adicionar functions se fornecidas
            if functions:
                kwargs["tools"] = [
                    {"type": "function", "function": f} for f in functions
                ]
                if function_call:
                    kwargs["tool_choice"] = function_call
            
            # Fazer chamada
            response = await self.client.chat.completions.create(**kwargs)
            
            # Calcular m√©tricas
            processing_time_ms = int((time.time() - start_time) * 1000)
            
            # Extrair resposta
            message = response.choices[0].message
            
            result = {
                "content": message.content,
                "role": message.role,
                "model_used": model,
                "tokens_input": response.usage.prompt_tokens,
                "tokens_output": response.usage.completion_tokens,
                "tokens_total": response.usage.total_tokens,
                "processing_time_ms": processing_time_ms,
                "finish_reason": response.choices[0].finish_reason
            }
            
            # Se usou function calling
            if hasattr(message, 'tool_calls') and message.tool_calls:
                result["tool_calls"] = [
                    {
                        "id": tc.id,
                        "function": tc.function.name,
                        "arguments": json.loads(tc.function.arguments)
                    }
                    for tc in message.tool_calls
                ]
            
            logger.info(
                f"‚úÖ LLM response: model={model}, "
                f"tokens={result['tokens_total']}, "
                f"time={processing_time_ms}ms"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erro na chamada LLM: {e}")
            raise
    
    def _select_model(self, messages: List[Dict[str, str]]) -> str:
        """
        Seleciona modelo baseado na complexidade da conversa.
        
        Regras:
        - GPT-4o-mini (80% dos casos): consultas simples, FAQ, classifica√ß√£o
        - GPT-4.1 (20%): vendas complexas, m√∫ltiplos produtos, recomenda√ß√µes
        """
        # Contar mensagens
        message_count = len([m for m in messages if m.get("role") == "user"])
        
        # Buscar indicadores de complexidade
        last_user_message = next(
            (m["content"] for m in reversed(messages) if m.get("role") == "user"),
            ""
        )
        
        complexity_indicators = [
            "recomend",
            "melhor",
            "compar",
            "diferen√ßa",
            "qual devo",
            "qual escolher",
            "vale a pena",
            "sugest"
        ]
        
        is_complex = any(
            indicator in last_user_message.lower()
            for indicator in complexity_indicators
        )
        
        # Decis√£o
        if is_complex or message_count > 5:
            return self.advanced_model
        
        return self.default_model
    
    # ========================================================================
    # STREAMING (para futuro)
    # ========================================================================
    
    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        callback: Optional[Callable] = None
    ):
        """
        Chat completion com streaming (para UI responsiva futura).
        
        Args:
            messages: Lista de mensagens
            model: Modelo a usar
            callback: Fun√ß√£o chamada a cada chunk
        """
        model = model or self.default_model
        
        stream = await self.client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True
        )
        
        full_response = ""
        
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                
                if callback:
                    await callback(content)
        
        return full_response


# ============================================================================
# PROMPT TEMPLATES
# ============================================================================

class PromptBuilder:
    """
    Constr√≥i prompts estruturados para a IA.
    """
    
    @staticmethod
    def build_system_prompt(context: Dict[str, Any]) -> str:
        """
        Constr√≥i system prompt com contexto do ERP.
        """
        tenant = context.get("tenant", {})
        cliente = context.get("cliente")
        produtos = context.get("produtos_relevantes", [])
        politicas = tenant.get("politicas", {})
        
        # Nome do bot
        bot_name = tenant.get("bot_name", "Assistente")
        
        # Tom da conversa
        tone_map = {
            "friendly": "Seja cordial, use emojis moderadamente üêæ, mostre empatia com os pets",
            "formal": "Seja profissional e objetivo, evite emojis",
            "casual": "Seja descontra√≠do e pr√≥ximo, use linguagem coloquial"
        }
        tone_instruction = tone_map.get(tenant.get("tone", "friendly"), tone_map["friendly"])
        
        # Montar prompt
        prompt = f"""Voc√™ √© {bot_name}, assistente de vendas de um pet shop.

REGRAS ABSOLUTAS:
1. NUNCA invente produtos que n√£o est√£o no cat√°logo fornecido
2. NUNCA ofere√ßa: {', '.join(politicas.get('proibido_vender', []))}
3. SEMPRE confirme endere√ßo antes de finalizar pedido
4. Se n√£o souber algo, seja honesto e ofere√ßa transferir para humano
5. Valores e estoque podem mudar - sempre mencione "consulte disponibilidade atual"

INFORMA√á√ïES DO CLIENTE:
{f"- Nome: {cliente['nome']}" if cliente else "- Cliente novo (n√£o identificado)"}
{f"- √öltimo pedido: R$ {cliente['ultimo_pedido']['valor']:.2f} em {cliente['ultimo_pedido']['data'][:10]}" if cliente and cliente.get('ultimo_pedido') else ""}
{f"- Cliente fiel ({cliente['total_compras_3m']} compras em 3 meses)" if cliente and cliente.get('cliente_fiel') else ""}

PRODUTOS DISPON√çVEIS:
{PromptBuilder._format_produtos(produtos)}

POL√çTICAS DA LOJA:
- Entrega m√≠nima: R$ {politicas.get('minimo_entrega', 50):.2f}
- Formas de pagamento: {', '.join(politicas.get('formas_pagamento', []))}
- √Åreas de entrega: {', '.join(politicas.get('areas_entrega', []))}

ESTILO DE COMUNICA√á√ÉO:
{tone_instruction}

IMPORTANTE:
- Sempre pergunte sobre o pet do cliente (nome, idade, porte, ra√ßa)
- Sugira produtos baseados nas necessidades do pet
- Se cliente perguntar sobre produto n√£o listado, diga que vai verificar disponibilidade
"""
        
        return prompt.strip()
    
    @staticmethod
    def _format_produtos(produtos: List[Dict[str, Any]]) -> str:
        """Formata lista de produtos para o prompt."""
        if not produtos:
            return "Nenhum produto espec√≠fico no momento (busque no sistema se necess√°rio)"
        
        formatted = []
        for p in produtos[:5]:  # M√°x 5 produtos
            linha = f"‚Ä¢ {p['nome']}"
            if p.get('preco'):
                linha += f" - R$ {p['preco']:.2f}"
            if p.get('estoque'):
                linha += f" ({p['estoque']} em estoque)"
            if p.get('descricao'):
                linha += f"\n  {p['descricao'][:100]}"
            formatted.append(linha)
        
        return "\n".join(formatted)
    
    @staticmethod
    def format_conversation_history(historico: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """
        Formata hist√≥rico de conversa para formato OpenAI.
        """
        messages = []
        
        for msg in historico:
            role = "user" if msg["tipo"] == "recebida" else "assistant"
            messages.append({
                "role": role,
                "content": msg["conteudo"]
            })
        
        return messages


# ============================================================================
# FUNCTION DEFINITIONS (para function calling)
# ============================================================================

AVAILABLE_FUNCTIONS = [
    {
        "name": "buscar_produto",
        "description": "Busca produtos no cat√°logo por nome, categoria ou descri√ß√£o",
        "parameters": {
            "type": "object",
            "properties": {
                "termo": {
                    "type": "string",
                    "description": "Termo de busca (ex: 'ra√ß√£o golden', 'shampoo para cachorro')"
                },
                "categoria": {
                    "type": "string",
                    "description": "Categoria espec√≠fica (opcional)",
                    "enum": ["Ra√ß√£o", "Brinquedo", "Higiene", "Acess√≥rio", "Medicamento"]
                }
            },
            "required": ["termo"]
        }
    },
    {
        "name": "consultar_estoque",
        "description": "Verifica disponibilidade em estoque de um produto espec√≠fico",
        "parameters": {
            "type": "object",
            "properties": {
                "produto_id": {
                    "type": "string",
                    "description": "ID do produto"
                }
            },
            "required": ["produto_id"]
        }
    },
    {
        "name": "calcular_frete",
        "description": "Calcula valor e prazo de entrega para um endere√ßo",
        "parameters": {
            "type": "object",
            "properties": {
                "cep": {
                    "type": "string",
                    "description": "CEP de entrega (ex: '01310-100')"
                },
                "valor_pedido": {
                    "type": "number",
                    "description": "Valor total do pedido"
                }
            },
            "required": ["cep"]
        }
    },
    {
        "name": "criar_pedido",
        "description": "Cria um novo pedido para o cliente",
        "parameters": {
            "type": "object",
            "properties": {
                "produtos": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "produto_id": {"type": "string"},
                            "quantidade": {"type": "integer"}
                        }
                    },
                    "description": "Lista de produtos e quantidades"
                },
                "forma_pagamento": {
                    "type": "string",
                    "enum": ["Dinheiro", "Pix", "Cart√£o D√©bito", "Cart√£o Cr√©dito"]
                },
                "endereco_entrega": {
                    "type": "string",
                    "description": "Endere√ßo completo de entrega"
                }
            },
            "required": ["produtos", "forma_pagamento"]
        }
    },
    {
        "name": "transferir_para_humano",
        "description": "Transfere conversa para atendente humano",
        "parameters": {
            "type": "object",
            "properties": {
                "motivo": {
                    "type": "string",
                    "description": "Motivo da transfer√™ncia"
                }
            },
            "required": ["motivo"]
        }
    }
]
