"""
Migration: Criar Event Store para Event Sourcing
================================================

Data: 2026-01-23
Fase: 5.2 - Event Store Enhanced

OBJETIVO:
Criar tabela domain_events para event sourcing completo
com suporte a replay determin√≠stico e auditoria.

CARACTER√çSTICAS:
- sequence_number: Ordem global monot√¥nica
- correlation_id: Rastrear fluxo completo
- causation_id: Evento que causou este evento
- √çndices otimizados para queries de replay
"""

import sqlite3
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


def upgrade(db_path: str = "petshop.db"):
    """
    Cria tabela domain_events com campos necess√°rios para event sourcing.
    
    CARACTER√çSTICAS:
    - AUTOINCREMENT garante sequence_number monot√¥nico
    - √çndices para queries de replay eficientes
    - Imutabilidade (apenas INSERT, nunca UPDATE/DELETE)
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        logger.info("üöÄ Iniciando migration: Event Store Enhanced")
        
        # 1. Criar tabela domain_events
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS domain_events (
                -- Identifica√ß√£o √∫nica do evento
                id TEXT PRIMARY KEY NOT NULL,
                
                -- Ordena√ß√£o global (CR√çTICO para replay)
                sequence_number INTEGER NOT NULL UNIQUE,
                
                -- Metadados do evento
                event_type TEXT NOT NULL,
                aggregate_id TEXT NOT NULL,
                aggregate_type TEXT NOT NULL,
                
                -- Multi-tenancy
                user_id INTEGER NOT NULL,
                
                -- Rastreabilidade
                correlation_id TEXT,  -- Rastrear fluxo completo (ex: toda uma venda)
                causation_id TEXT,    -- Evento que causou este evento
                
                -- Dados do evento
                payload TEXT NOT NULL,  -- JSON serializado
                metadata TEXT,          -- JSON com dados t√©cnicos
                
                -- Timestamp
                created_at TEXT NOT NULL,
                
                -- Constraints
                FOREIGN KEY (user_id) REFERENCES users(id)
            )
        """)
        logger.info("‚úÖ Tabela domain_events criada")
        
        # 2. Criar √≠ndices para queries de replay
        indexes = [
            # √çndice principal: ordena√ß√£o global
            ("idx_domain_events_sequence", "sequence_number"),
            
            # Replay por tenant
            ("idx_domain_events_user_seq", "user_id, sequence_number"),
            
            # Replay por tipo de evento
            ("idx_domain_events_type_seq", "event_type, sequence_number"),
            
            # Replay por agregado
            ("idx_domain_events_aggregate", "aggregate_id, sequence_number"),
            
            # Rastreabilidade
            ("idx_domain_events_correlation", "correlation_id"),
            
            # Busca por data
            ("idx_domain_events_created_at", "created_at"),
        ]
        
        for index_name, columns in indexes:
            cursor.execute(f"""
                CREATE INDEX IF NOT EXISTS {index_name}
                ON domain_events ({columns})
            """)
            logger.info(f"‚úÖ √çndice {index_name} criado")
        
        # 3. Criar sequence para sequence_number (SQLite usa AUTOINCREMENT)
        # Nota: Em SQLite, AUTOINCREMENT garante monotonia mesmo com DELETEs
        # Em PostgreSQL, usar√≠amos uma SEQUENCE separada
        
        conn.commit()
        logger.info("‚úÖ Migration Event Store Enhanced conclu√≠da com sucesso!")
        
        # 4. Validar estrutura
        cursor.execute("PRAGMA table_info(domain_events)")
        columns = cursor.fetchall()
        logger.info(f"üìä Colunas criadas: {[col[1] for col in columns]}")
        
        return True
        
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå Erro na migration: {str(e)}", exc_info=True)
        raise
    finally:
        conn.close()


def downgrade(db_path: str = "petshop.db"):
    """
    Remove tabela domain_events (USE COM CUIDADO!).
    
    ATEN√á√ÉO: Esta opera√ß√£o √© DESTRUTIVA e IRREVERS√çVEL!
    Apenas usar em ambiente de desenvolvimento.
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        logger.warning("‚ö†Ô∏è  DOWNGRADE: Removendo tabela domain_events")
        
        cursor.execute("DROP TABLE IF EXISTS domain_events")
        
        conn.commit()
        logger.info("‚úÖ Downgrade conclu√≠do")
        
        return True
        
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå Erro no downgrade: {str(e)}", exc_info=True)
        raise
    finally:
        conn.close()


def validate(db_path: str = "petshop.db"):
    """
    Valida que a migration foi aplicada corretamente.
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        # Verificar se tabela existe
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='domain_events'
        """)
        if not cursor.fetchone():
            logger.error("‚ùå Tabela domain_events n√£o existe!")
            return False
        
        # Verificar colunas obrigat√≥rias
        cursor.execute("PRAGMA table_info(domain_events)")
        columns = {col[1] for col in cursor.fetchall()}
        
        required_columns = {
            'id', 'sequence_number', 'event_type', 'aggregate_id',
            'aggregate_type', 'user_id', 'correlation_id', 'causation_id',
            'payload', 'metadata', 'created_at'
        }
        
        if not required_columns.issubset(columns):
            missing = required_columns - columns
            logger.error(f"‚ùå Colunas faltando: {missing}")
            return False
        
        # Verificar √≠ndices
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='index' AND tbl_name='domain_events'
        """)
        indexes = {row[0] for row in cursor.fetchall()}
        
        expected_indexes = {
            'idx_domain_events_sequence',
            'idx_domain_events_user_seq',
            'idx_domain_events_type_seq',
            'idx_domain_events_aggregate',
            'idx_domain_events_correlation',
            'idx_domain_events_created_at',
        }
        
        if not expected_indexes.issubset(indexes):
            missing = expected_indexes - indexes
            logger.warning(f"‚ö†Ô∏è  √çndices faltando: {missing}")
            # N√£o √© cr√≠tico, apenas aviso
        
        logger.info("‚úÖ Valida√ß√£o da migration passou!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro na valida√ß√£o: {str(e)}", exc_info=True)
        return False
    finally:
        conn.close()


if __name__ == "__main__":
    # Configurar logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Executar migration
    print("=" * 60)
    print("Migration: Event Store Enhanced")
    print("=" * 60)
    
    db_path = Path(__file__).parent.parent / "petshop.db"
    
    if upgrade(str(db_path)):
        print("\n‚úÖ Migration aplicada com sucesso!")
        
        if validate(str(db_path)):
            print("‚úÖ Valida√ß√£o passou!")
        else:
            print("‚ö†Ô∏è  Valida√ß√£o falhou (verificar logs)")
    else:
        print("\n‚ùå Falha ao aplicar migration!")
